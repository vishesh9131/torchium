

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>torch.nn.modules.loss &mdash; Torchium 0.1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=14fc6294" />
      <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=14fc6294" />

  
    <link rel="shortcut icon" href="../../../../_static/logo.png"/>
    <link rel="canonical" href="https://vishesh9131.github.io/torchium/_modules/torch/nn/modules/loss.html" />
      <script src="../../../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../../../_static/documentation_options.js?v=01f34227"></script>
      <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Torchium
              <img src="../../../../_static/logo.png" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/quickstart.html">Quickstart Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/advanced_usage.html">Advanced Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/domain_specific_usage.html">Domain-Specific Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/performance_guide.html">Performance Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/cuda_integration.html">CUDA Integration and Custom Kernels</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tutorials/cython_optimizations.html">Cython Optimizations</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/optimizers.html">Optimizers API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api/losses.html">Loss Functions API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/computer_vision.html">Computer Vision Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/nlp.html">Natural Language Processing Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/generative_models.html">Generative Models Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/optimization_comparison.html">Optimization Comparison Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../examples/benchmarks.html">Benchmarks Examples</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Torchium</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">torch.nn.modules.loss</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for torch.nn.modules.loss</h1><div class="highlight"><pre>
<span></span><span class="c1"># mypy: allow-untyped-defs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing_extensions</span><span class="w"> </span><span class="kn">import</span> <span class="n">deprecated</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">_reduction</span> <span class="k">as</span> <span class="n">_Reduction</span><span class="p">,</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.distance</span><span class="w"> </span><span class="kn">import</span> <span class="n">PairwiseDistance</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.module</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;L1Loss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NLLLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;NLLLoss2d&quot;</span><span class="p">,</span>
    <span class="s2">&quot;PoissonNLLLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;GaussianNLLLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;KLDivLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MSELoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BCELoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;BCEWithLogitsLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HingeEmbeddingLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiLabelMarginLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SmoothL1Loss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;HuberLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;SoftMarginLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CrossEntropyLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiLabelSoftMarginLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CosineEmbeddingLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MarginRankingLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;MultiMarginLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TripletMarginLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;TripletMarginWithDistanceLoss&quot;</span><span class="p">,</span>
    <span class="s2">&quot;CTCLoss&quot;</span><span class="p">,</span>
<span class="p">]</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>


<span class="k">class</span><span class="w"> </span><span class="nc">_WeightedLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>


<div class="viewcode-block" id="L1Loss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.L1Loss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">L1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean absolute error (MAE) between each element in</span>
<span class="sd">    the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`N` elements each.</span>

<span class="sd">    The sum operation still operates over all the elements, and divides by :math:`N`.</span>

<span class="sd">    The division by :math:`N` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Supports real-valued and complex-valued inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(*)`, same shape as the input.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="L1Loss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.L1Loss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="L1Loss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.L1Loss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>
</div>



<div class="viewcode-block" id="NLLLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.NLLLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">NLLLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss. It is useful to train a classification</span>
<span class="sd">    problem with `C` classes.</span>

<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning</span>
<span class="sd">    weight to each of the classes. This is particularly useful when you have an</span>
<span class="sd">    unbalanced training set.</span>

<span class="sd">    The `input` given through a forward call is expected to contain</span>
<span class="sd">    log-probabilities of each class. `input` has to be a Tensor of size either</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case. The latter is useful for</span>
<span class="sd">    higher dimension inputs, such as computing NLL loss per-pixel for 2D images.</span>

<span class="sd">    Obtaining log-probabilities in a neural network is easily achieved by</span>
<span class="sd">    adding a  `LogSoftmax`  layer in the last layer of your network.</span>
<span class="sd">    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra</span>
<span class="sd">    layer.</span>

<span class="sd">    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`</span>
<span class="sd">    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts</span>
<span class="sd">    this class index (this index may not necessarily be in the class range).</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \\</span>
<span class="sd">        l_n = - w_{y_n} x_{n,y_n}, \\</span>
<span class="sd">        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</span>

<span class="sd">    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and</span>
<span class="sd">    :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;</span>
<span class="sd">            \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">            \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``None``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When</span>
<span class="sd">            :attr:`size_average` is ``True``, the loss is averaged over</span>
<span class="sd">            non-ignored targets.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``None``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape::</span>
<span class="sd">        - Input: :math:`(N, C)` or :math:`(C)`, where `C = number of classes`, `N = batch size`, or</span>
<span class="sd">          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, where each value is</span>
<span class="sd">          :math:`0 \leq \text{targets}[i] \leq C-1`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of</span>
<span class="sd">          K-dimensional loss.</span>
<span class="sd">        - Output: If :attr:`reduction` is ``&#39;none&#39;``, shape :math:`(N)` or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of K-dimensional loss.</span>
<span class="sd">          Otherwise, scalar.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; log_softmax = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; loss_fn = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; # input to NLLLoss is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target must have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; loss = loss_fn(log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)</span>
<span class="sd">        &gt;&gt;&gt; N, C = 5, 4</span>
<span class="sd">        &gt;&gt;&gt; loss_fn = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)</span>
<span class="sd">        &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; log_softmax = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; # output of conv forward is of shape [N, C, 8, 8]</span>
<span class="sd">        &gt;&gt;&gt; output = log_softmax(conv(data))</span>
<span class="sd">        &gt;&gt;&gt; # each element in target must have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>
<span class="sd">        &gt;&gt;&gt; # input to NLLLoss is of size N x C x height (8) x width (8)</span>
<span class="sd">        &gt;&gt;&gt; loss = loss_fn(output, target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>

<div class="viewcode-block" id="NLLLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.NLLLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span></div>


<div class="viewcode-block" id="NLLLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.NLLLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<span class="nd">@deprecated</span><span class="p">(</span>
    <span class="s2">&quot;`NLLLoss2d` has been deprecated. &quot;</span>
    <span class="s2">&quot;Please use `NLLLoss` instead as a drop-in replacement and see &quot;</span>
    <span class="s2">&quot;https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.&quot;</span><span class="p">,</span>
    <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">NLLLoss2d</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="PoissonNLLLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.PoissonNLLLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">PoissonNLLLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Negative log likelihood loss with Poisson distribution of target.</span>

<span class="sd">    The loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{target} \sim \mathrm{Poisson}(\text{input})</span>

<span class="sd">        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})</span>
<span class="sd">                                    + \log(\text{target!})</span>

<span class="sd">    The last term can be omitted or approximated with Stirling formula. The</span>
<span class="sd">    approximation is used for target values more than 1. For targets less or</span>
<span class="sd">    equal to 1 zeros are added to the loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_input (bool, optional): if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target}*\text{input}`, if ``False`` the loss is</span>
<span class="sd">            :math:`\text{input} - \text{target}*\log(\text{input}+\text{eps})`.</span>
<span class="sd">        full (bool, optional): whether to compute full loss, i. e. to add the</span>
<span class="sd">            Stirling approximation term</span>

<span class="sd">            .. math::</span>
<span class="sd">                \text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input = False`. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.PoissonNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(log_input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar by default. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`,</span>
<span class="sd">          the same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;log_input&quot;</span><span class="p">,</span> <span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>

<div class="viewcode-block" id="PoissonNLLLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.PoissonNLLLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_input</span> <span class="o">=</span> <span class="n">log_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span></div>


<div class="viewcode-block" id="PoissonNLLLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.PoissonNLLLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">poisson_nll_loss</span><span class="p">(</span>
            <span class="n">log_input</span><span class="p">,</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="n">log_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_input</span><span class="p">,</span>
            <span class="n">full</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="GaussianNLLLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.GaussianNLLLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">GaussianNLLLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Gaussian negative log likelihood loss.</span>

<span class="sd">    The targets are treated as samples from Gaussian distributions with</span>
<span class="sd">    expectations and variances predicted by the neural network. For a</span>
<span class="sd">    ``target`` tensor modelled as having Gaussian distribution with a tensor</span>
<span class="sd">    of expectations ``input`` and a tensor of positive variances ``var`` the loss is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{1}{2}\left(\log\left(\text{max}\left(\text{var},</span>
<span class="sd">        \ \text{eps}\right)\right) + \frac{\left(\text{input} - \text{target}\right)^2}</span>
<span class="sd">        {\text{max}\left(\text{var}, \ \text{eps}\right)}\right) + \text{const.}</span>

<span class="sd">    where :attr:`eps` is used for stability. By default, the constant term of</span>
<span class="sd">    the loss function is omitted unless :attr:`full` is ``True``. If ``var`` is not the same</span>
<span class="sd">    size as ``input`` (due to a homoscedastic assumption), it must either have a final dimension</span>
<span class="sd">    of 1 or have one fewer dimension (with all other sizes being the same) for correct broadcasting.</span>

<span class="sd">    Args:</span>
<span class="sd">        full (bool, optional): include the constant term in the loss</span>
<span class="sd">            calculation. Default: ``False``.</span>
<span class="sd">        eps (float, optional): value used to clamp ``var`` (see note below), for</span>
<span class="sd">            stability. Default: 1e-6.</span>
<span class="sd">        reduction (str, optional): specifies the reduction to apply to the</span>
<span class="sd">            output:``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction</span>
<span class="sd">            will be applied, ``&#39;mean&#39;``: the output is the average of all batch</span>
<span class="sd">            member losses, ``&#39;sum&#39;``: the output is the sum of all batch member</span>
<span class="sd">            losses. Default: ``&#39;mean&#39;``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` or :math:`(*)` where :math:`*` means any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input</span>
<span class="sd">          but with one dimension equal to 1 (to allow for broadcasting)</span>
<span class="sd">        - Var: :math:`(N, *)` or :math:`(*)`, same shape as the input, or same shape as the input but</span>
<span class="sd">          with one dimension equal to 1, or same shape as the input but with one fewer</span>
<span class="sd">          dimension (to allow for broadcasting), or a scalar value</span>
<span class="sd">        - Output: scalar if :attr:`reduction` is ``&#39;mean&#39;`` (default) or</span>
<span class="sd">          ``&#39;sum&#39;``. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N, *)`, same</span>
<span class="sd">          shape as the input</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.GaussianNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; var = torch.ones(5, 2, requires_grad=True)  # heteroscedastic</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target, var)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.GaussianNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; var = torch.ones(5, 1, requires_grad=True)  # homoscedastic</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target, var)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Note:</span>
<span class="sd">        The clamping of ``var`` is ignored with respect to autograd, and so the</span>
<span class="sd">        gradients are unaffected by it.</span>

<span class="sd">    Reference:</span>
<span class="sd">        Nix, D. A. and Weigend, A. S., &quot;Estimating the mean and variance of the</span>
<span class="sd">        target probability distribution&quot;, Proceedings of 1994 IEEE International</span>
<span class="sd">        Conference on Neural Networks (ICNN&#39;94), Orlando, FL, USA, 1994, pp. 55-60</span>
<span class="sd">        vol.1, doi: 10.1109/ICNN.1994.374138.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;full&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>

<div class="viewcode-block" id="GaussianNLLLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.GaussianNLLLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span></div>


<div class="viewcode-block" id="GaussianNLLLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.GaussianNLLLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">var</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gaussian_nll_loss</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="KLDivLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.KLDivLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Kullback-Leibler divergence loss.</span>

<span class="sd">    For tensors of the same shape :math:`y_{\text{pred}},\ y_{\text{true}}`,</span>
<span class="sd">    where :math:`y_{\text{pred}}` is the :attr:`input` and :math:`y_{\text{true}}` is the</span>
<span class="sd">    :attr:`target`, we define the **pointwise KL-divergence** as</span>

<span class="sd">    .. math::</span>

<span class="sd">        L(y_{\text{pred}},\ y_{\text{true}})</span>
<span class="sd">            = y_{\text{true}} \cdot \log \frac{y_{\text{true}}}{y_{\text{pred}}}</span>
<span class="sd">            = y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})</span>

<span class="sd">    To avoid underflow issues when computing this quantity, this loss expects the argument</span>
<span class="sd">    :attr:`input` in the log-space. The argument :attr:`target` may also be provided in the</span>
<span class="sd">    log-space if :attr:`log_target`\ `= True`.</span>

<span class="sd">    To summarise, this function is roughly equivalent to computing</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        if not log_target:  # default</span>
<span class="sd">            loss_pointwise = target * (target.log() - input)</span>
<span class="sd">        else:</span>
<span class="sd">            loss_pointwise = target.exp() * (target - input)</span>

<span class="sd">    and then reducing this result depending on the argument :attr:`reduction` as</span>

<span class="sd">    .. code-block:: python</span>

<span class="sd">        if reduction == &quot;mean&quot;:  # default</span>
<span class="sd">            loss = loss_pointwise.mean()</span>
<span class="sd">        elif reduction == &quot;batchmean&quot;:  # mathematically correct</span>
<span class="sd">            loss = loss_pointwise.sum() / input.size(0)</span>
<span class="sd">        elif reduction == &quot;sum&quot;:</span>
<span class="sd">            loss = loss_pointwise.sum()</span>
<span class="sd">        else:  # reduction == &quot;none&quot;</span>
<span class="sd">            loss = loss_pointwise</span>

<span class="sd">    .. note::</span>
<span class="sd">        As all the other losses in PyTorch, this function expects the first argument,</span>
<span class="sd">        :attr:`input`, to be the output of the model (e.g. the neural network)</span>
<span class="sd">        and the second, :attr:`target`, to be the observations in the dataset.</span>
<span class="sd">        This differs from the standard mathematical notation :math:`KL(P\ ||\ Q)` where</span>
<span class="sd">        :math:`P` denotes the distribution of the observations and :math:`Q` denotes the model.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        :attr:`reduction`\ `= &quot;mean&quot;` doesn&#39;t return the true KL divergence value, please use</span>
<span class="sd">        :attr:`reduction`\ `= &quot;batchmean&quot;` which aligns with the mathematical definition.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to `False`, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is `False`. Default: `True`</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is `False`, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: `True`</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output. Default: `&quot;mean&quot;`</span>
<span class="sd">        log_target (bool, optional): Specifies whether `target` is the log space. Default: `False`</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar by default. If :attr:`reduction` is `&#39;none&#39;`, then :math:`(*)`,</span>
<span class="sd">          same shape as the input.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # input should be a distribution in the log space</span>
<span class="sd">        &gt;&gt;&gt; input = F.log_softmax(torch.randn(3, 5, requires_grad=True), dim=1)</span>
<span class="sd">        &gt;&gt;&gt; # Sample a batch of distributions. Usually this would come from the dataset</span>
<span class="sd">        &gt;&gt;&gt; target = F.softmax(torch.rand(3, 5), dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = kl_loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; kl_loss = nn.KLDivLoss(reduction=&quot;batchmean&quot;, log_target=True)</span>
<span class="sd">        &gt;&gt;&gt; log_target = F.log_softmax(torch.rand(3, 5), dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = kl_loss(input, log_target)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="KLDivLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.KLDivLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">log_target</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_target</span> <span class="o">=</span> <span class="n">log_target</span></div>


<div class="viewcode-block" id="KLDivLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.KLDivLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_target</span>
        <span class="p">)</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean squared error (squared L2 norm) between</span>
<span class="sd">    each element in the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left( x_n - y_n \right)^2,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`N` elements each.</span>

<span class="sd">    The mean operation still operates over all the elements, and divides by :math:`N`.</span>

<span class="sd">    The division by :math:`N` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="BCELoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCELoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">BCELoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the Binary Cross Entropy between the target and</span>
<span class="sd">    the input probabilities:</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets :math:`y` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be</span>
<span class="sd">    mathematically undefined in the above loss equation. PyTorch chooses to set</span>
<span class="sd">    :math:`\log (0) = -\infty`, since :math:`\lim_{x\to 0} \log (x) = -\infty`.</span>
<span class="sd">    However, an infinite term in the loss equation is not desirable for several reasons.</span>

<span class="sd">    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be</span>
<span class="sd">    multiplying 0 with infinity. Secondly, if we have an infinite loss value, then</span>
<span class="sd">    we would also have an infinite term in our gradient, since</span>
<span class="sd">    :math:`\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty`.</span>
<span class="sd">    This would make BCELoss&#39;s backward method nonlinear with respect to :math:`x_n`,</span>
<span class="sd">    and using it for things like linear regression would not be straight-forward.</span>

<span class="sd">    Our solution is that BCELoss clamps its log function outputs to be greater than</span>
<span class="sd">    or equal to -100. This way, we can always have a finite loss value and a linear</span>
<span class="sd">    backward method.</span>


<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.rand(3, 2, requires_grad=False)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="BCELoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCELoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="BCELoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCELoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="BCEWithLogitsLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCEWithLogitsLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This loss combines a `Sigmoid` layer and the `BCELoss` in one single</span>
<span class="sd">    class. This version is more numerically stable than using a plain `Sigmoid`</span>
<span class="sd">    followed by a `BCELoss` as, by combining the operations into one layer,</span>
<span class="sd">    we take advantage of the log-sum-exp trick for numerical stability.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)</span>
<span class="sd">        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets `t[i]` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    It&#39;s possible to trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad</span>
<span class="sd">        l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})</span>
<span class="sd">        + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</span>

<span class="sd">    where :math:`c` is the class number (:math:`c &gt; 1` for multi-label binary classification,</span>
<span class="sd">    :math:`c = 1` for single-label binary classification),</span>
<span class="sd">    :math:`n` is the number of the sample in the batch and</span>
<span class="sd">    :math:`p_c` is the weight of the positive answer for the class :math:`c`.</span>

<span class="sd">    :math:`p_c &gt; 1` increases the recall, :math:`p_c &lt; 1` increases the precision.</span>

<span class="sd">    For example, if a dataset contains 100 positive and 300 negative examples of a single class,</span>
<span class="sd">    then ``pos_weight`` for the class should be equal to :math:`\frac{300}{100}=3`.</span>
<span class="sd">    The loss would act as if the dataset contains :math:`3\times 100=300` positive examples.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10</span>
<span class="sd">        &gt;&gt;&gt; output = torch.full([10, 64], 1.5)  # A prediction (logit)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1</span>
<span class="sd">        &gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(1.5))</span>
<span class="sd">        tensor(0.20...)</span>

<span class="sd">    In the above example, the ``pos_weight`` tensor&#39;s elements correspond to the 64 distinct classes</span>
<span class="sd">    in a multi-label binary classification scenario. Each element in ``pos_weight`` is designed to adjust the</span>
<span class="sd">    loss function based on the imbalance between negative and positive samples for the respective class.</span>
<span class="sd">    This approach is useful in datasets with varying levels of class imbalance, ensuring that the loss</span>
<span class="sd">    calculation accurately accounts for the distribution in each class.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples to be broadcasted with target.</span>
<span class="sd">            Must be a tensor with equal size along the class dimension to the number of classes.</span>
<span class="sd">            Pay close attention to PyTorch&#39;s broadcasting semantics in order to achieve the desired</span>
<span class="sd">            operations. For a target of size [B, C, H, W] (where B is batch size) pos_weight of</span>
<span class="sd">            size [B, C, H, W] will apply different pos_weights to each element of the batch or</span>
<span class="sd">            [C, H, W] the same pos_weights across the batch. To apply the same positive weight</span>
<span class="sd">            along all spacial dimensions for a 2D multi-class target [C, H, W] use: [C, 1, 1].</span>
<span class="sd">            Default: ``None``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="BCEWithLogitsLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCEWithLogitsLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">pos_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;weight&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pos_weight&quot;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span></div>


<div class="viewcode-block" id="BCEWithLogitsLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.BCEWithLogitsLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">pos_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="HingeEmbeddingLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.HingeEmbeddingLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>
<span class="sd">    This is usually used for measuring whether two inputs are similar or</span>
<span class="sd">    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically</span>
<span class="sd">    used for learning nonlinear embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">            x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">            \max \{0, margin - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of `1`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation</span>
<span class="sd">          operates over all the elements.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the input</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

<div class="viewcode-block" id="HingeEmbeddingLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.HingeEmbeddingLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span></div>


<div class="viewcode-block" id="HingeEmbeddingLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.HingeEmbeddingLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="MultiLabelMarginLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelMarginLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiLabelMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>

<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>

<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>

<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`</span>
<span class="sd">          is the number of classes.</span>
<span class="sd">        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])</span>
<span class="sd">        &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1</span>
<span class="sd">        &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])</span>
<span class="sd">        &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="sd">        &gt;&gt;&gt; loss(x, y)</span>
<span class="sd">        tensor(0.85...)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="MultiLabelMarginLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelMarginLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiLabelMarginLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelMarginLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below beta and an L1 term otherwise.</span>
<span class="sd">    It is less sensitive to outliers than :class:`torch.nn.MSELoss` and in some cases</span>
<span class="sd">    prevents exploding gradients (e.g. see the paper `Fast R-CNN`_ by Ross Girshick).</span>

<span class="sd">    For a batch of size :math:`N`, the unreduced loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1, ..., l_N\}^T</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        0.5 (x_n - y_n)^2 / beta, &amp; \text{if } |x_n - y_n| &lt; beta \\</span>
<span class="sd">        |x_n - y_n| - 0.5 * beta, &amp; \text{otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        Smooth L1 loss can be seen as exactly :class:`L1Loss`, but with the :math:`|x - y| &lt; beta`</span>
<span class="sd">        portion replaced with a quadratic function such that its slope is 1 at :math:`|x - y| = beta`.</span>
<span class="sd">        The quadratic segment smooths the L1 loss near :math:`|x - y| = 0`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Smooth L1 loss is closely related to :class:`HuberLoss`, being</span>
<span class="sd">        equivalent to :math:`huber(x, y) / beta` (note that Smooth L1&#39;s beta hyper-parameter is</span>
<span class="sd">        also known as delta for Huber). This leads to the following differences:</span>

<span class="sd">        * As beta -&gt; 0, Smooth L1 loss converges to :class:`L1Loss`, while :class:`HuberLoss`</span>
<span class="sd">          converges to a constant 0 loss. When beta is 0, Smooth L1 loss is equivalent to L1 loss.</span>
<span class="sd">        * As beta -&gt; :math:`+\infty`, Smooth L1 loss converges to a constant 0 loss, while</span>
<span class="sd">          :class:`HuberLoss` converges to :class:`MSELoss`.</span>
<span class="sd">        * For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant slope of 1.</span>
<span class="sd">          For :class:`HuberLoss`, the slope of the L1 segment is beta.</span>

<span class="sd">    .. _`Fast R-CNN`: https://arxiv.org/abs/1504.08083</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        beta (float, optional): Specifies the threshold at which to change between L1 and L2 loss.</span>
<span class="sd">            The value must be non-negative. Default: 1.0</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">HuberLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below delta and a delta-scaled L1 term otherwise.</span>
<span class="sd">    This loss combines advantages of both :class:`L1Loss` and :class:`MSELoss`; the</span>
<span class="sd">    delta-scaled L1 region makes the loss less sensitive to outliers than :class:`MSELoss`,</span>
<span class="sd">    while the L2 region provides smoothness over :class:`L1Loss` near 0. See</span>
<span class="sd">    `Huber loss &lt;https://en.wikipedia.org/wiki/Huber_loss&gt;`_ for more information.</span>

<span class="sd">    For a batch of size :math:`N`, the unreduced loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1, ..., l_N\}^T</span>

<span class="sd">    with</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">        0.5 (x_n - y_n)^2, &amp; \text{if } |x_n - y_n| &lt; delta \\</span>
<span class="sd">        delta * (|x_n - y_n| - 0.5 * delta), &amp; \text{otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    If `reduction` is not `none`, then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        When delta is set to 1, this loss is equivalent to :class:`SmoothL1Loss`.</span>
<span class="sd">        In general, this loss differs from :class:`SmoothL1Loss` by a factor of delta (AKA beta</span>
<span class="sd">        in Smooth L1).</span>
<span class="sd">        See :class:`SmoothL1Loss` for additional discussion on the differences in behavior</span>
<span class="sd">        between the two losses.</span>

<span class="sd">    Args:</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>
<span class="sd">        delta (float, optional): Specifies the threshold at which to change between delta-scaled L1 and L2 loss.</span>
<span class="sd">            The value must be positive.  Default: 1.0</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="s2">&quot;delta&quot;</span><span class="p">]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">huber_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">delta</span><span class="p">)</span>


<div class="viewcode-block" id="SoftMarginLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.SoftMarginLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)`, where :math:`*` means any number of dimensions.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(*)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="SoftMarginLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.SoftMarginLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="SoftMarginLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.SoftMarginLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion computes the cross entropy loss between input logits</span>
<span class="sd">    and target.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    The `input` is expected to contain the unnormalized logits for each class (which do `not` need</span>
<span class="sd">    to be positive or sum to 1, in general).</span>
<span class="sd">    `input` has to be a Tensor of size :math:`(C)` for unbatched input,</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1` for the</span>
<span class="sd">    `K`-dimensional case. The last being useful for higher dimension inputs, such</span>
<span class="sd">    as computing cross entropy loss per-pixel for 2D images.</span>

<span class="sd">    The `target` that this criterion expects should contain either:</span>

<span class="sd">    - Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if</span>
<span class="sd">      `ignore_index` is specified, this loss also accepts this class index (this index</span>
<span class="sd">      may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`</span>
<span class="sd">      set to ``&#39;none&#39;``) loss for this case can be described as:</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}</span>
<span class="sd">          \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}</span>

<span class="sd">      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,</span>
<span class="sd">      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as</span>
<span class="sd">      :math:`d_1, ..., d_k` for the `K`-dimensional case. If</span>
<span class="sd">      :attr:`reduction` is not ``&#39;none&#39;`` (default ``&#39;mean&#39;``), then</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}} l_n, &amp;</span>
<span class="sd">               \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">                \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">                \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">      Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`</span>
<span class="sd">      on an input, followed by :class:`~torch.nn.NLLLoss`.</span>

<span class="sd">    - Probabilities for each class; useful when labels beyond a single class per minibatch item</span>
<span class="sd">      are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with</span>
<span class="sd">      :attr:`reduction` set to ``&#39;none&#39;``) loss for this case can be described as:</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}</span>

<span class="sd">      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,</span>
<span class="sd">      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as</span>
<span class="sd">      :math:`d_1, ..., d_k` for the `K`-dimensional case. If</span>
<span class="sd">      :attr:`reduction` is not ``&#39;none&#39;`` (default ``&#39;mean&#39;``), then</span>

<span class="sd">      .. math::</span>
<span class="sd">          \ell(x, y) = \begin{cases}</span>
<span class="sd">              \frac{\sum_{n=1}^N l_n}{N}, &amp;</span>
<span class="sd">               \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">                \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">                \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">            \end{cases}</span>

<span class="sd">    .. note::</span>
<span class="sd">        The performance of this criterion is generally better when `target` contains class</span>
<span class="sd">        indices, as this allows for optimized computation. Consider providing `target` as</span>
<span class="sd">        class probabilities only when a single class label per minibatch item is too restrictive.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each class.</span>
<span class="sd">            If given, has to be a Tensor of size `C`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Note that</span>
<span class="sd">            :attr:`ignore_index` is only applicable when the target contains class indices.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount</span>
<span class="sd">            of smoothing when computing the loss, where 0.0 means no smoothing. The targets</span>
<span class="sd">            become a mixture of the original ground truth and a uniform distribution as described in</span>
<span class="sd">            `Rethinking the Inception Architecture for Computer Vision &lt;https://arxiv.org/abs/1512.00567&gt;`__. Default: :math:`0.0`.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with</span>
<span class="sd">          :math:`K \geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`. The</span>
<span class="sd">          target data type is required to be long when using class indices. If containing class probabilities, the</span>
<span class="sd">          target must be the same shape input, and each value should be between :math:`[0, 1]`. This means the target</span>
<span class="sd">          data type is required to be float when using class probabilities.</span>
<span class="sd">        - Output: If reduction is &#39;none&#39;, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.</span>


<span class="sd">        where:</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{aligned}</span>
<span class="sd">                C ={} &amp; \text{number of classes} \\</span>
<span class="sd">                N ={} &amp; \text{batch size} \\</span>
<span class="sd">            \end{aligned}</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; # Example of target with class indices</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Example of target with class probabilities</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5).softmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ignore_index&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">,</span> <span class="s2">&quot;label_smoothing&quot;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">label_smoothing</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
        <span class="n">label_smoothing</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span> <span class="o">=</span> <span class="n">label_smoothing</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
            <span class="n">label_smoothing</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_smoothing</span><span class="p">,</span>
        <span class="p">)</span>


<div class="viewcode-block" id="MultiLabelSoftMarginLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelSoftMarginLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiLabelSoftMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-label one-versus-all</span>
<span class="sd">    loss based on max-entropy, between input :math:`x` and target :math:`y` of size</span>
<span class="sd">    :math:`(N, C)`.</span>
<span class="sd">    For each sample in the minibatch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})</span>
<span class="sd">                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</span>

<span class="sd">    where :math:`i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}`,</span>
<span class="sd">    :math:`y[i] \in \left\{0, \; 1\right\}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.</span>
<span class="sd">        - Target: :math:`(N, C)`, label targets must have the same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;reduction&quot;</span><span class="p">]</span>

<div class="viewcode-block" id="MultiLabelSoftMarginLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelSoftMarginLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="MultiLabelSoftMarginLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiLabelSoftMarginLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_soft_margin_loss</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="CosineEmbeddingLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CosineEmbeddingLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given input tensors</span>
<span class="sd">    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.</span>
<span class="sd">    Use (:math:`y=1`) to maximize the cosine similarity of two inputs, and (:math:`y=-1`) otherwise.</span>
<span class="sd">    This is typically used for learning nonlinear</span>
<span class="sd">    embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for each sample is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\</span>
<span class="sd">        \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,</span>
<span class="sd">            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the</span>
<span class="sd">            default value is :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input1: :math:`(N, D)` or :math:`(D)`, where `N` is the batch size and `D` is the embedding dimension.</span>
<span class="sd">        - Input2: :math:`(N, D)` or :math:`(D)`, same shape as Input1.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`.</span>
<span class="sd">        - Output: If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`, otherwise scalar.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.CosineEmbeddingLoss()</span>
<span class="sd">        &gt;&gt;&gt; input1 = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; input2 = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.ones(3)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

<div class="viewcode-block" id="CosineEmbeddingLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CosineEmbeddingLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span></div>


<div class="viewcode-block" id="CosineEmbeddingLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CosineEmbeddingLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span>
            <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="MarginRankingLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MarginRankingLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MarginRankingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given</span>
<span class="sd">    inputs :math:`x1`, :math:`x2`, two 1D mini-batch or 0D `Tensors`,</span>
<span class="sd">    and a label 1D mini-batch or 0D `Tensor` :math:`y` (containing 1 or -1).</span>

<span class="sd">    If :math:`y = 1` then it assumed the first input should be ranked higher</span>
<span class="sd">    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.</span>

<span class="sd">    The loss function for each pair of samples in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input1: :math:`(N)` or :math:`()` where `N` is the batch size.</span>
<span class="sd">        - Input2: :math:`(N)` or :math:`()`, same shape as the Input1.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, same shape as the inputs.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;`` and Input size is not :math:`()`, then :math:`(N)`.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MarginRankingLoss()</span>
<span class="sd">        &gt;&gt;&gt; input1 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; input2 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3).sign()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

<div class="viewcode-block" id="MarginRankingLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MarginRankingLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span></div>


<div class="viewcode-block" id="MarginRankingLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MarginRankingLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span>
            <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="MultiMarginLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiMarginLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i])^p}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`i \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>

<span class="sd">    Optionally, you can give non-equal weighting on the classes by passing</span>
<span class="sd">    a 1D :attr:`weight` tensor into the constructor.</span>

<span class="sd">    The loss function then becomes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i w[y] * \max(0, \text{margin} - x[y] + x[i])^p}{\text{x.size}(0)}</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`</span>
<span class="sd">            are the only supported values.</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`1`.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` or :math:`(C)`, where :math:`N` is the batch size and :math:`C` is the number of classes.</span>
<span class="sd">        - Target: :math:`(N)` or :math:`()`, where each value is :math:`0 \leq \text{targets}[i] \leq C-1`.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the target.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MultiMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = torch.tensor([[0.1, 0.2, 0.4, 0.8]])</span>
<span class="sd">        &gt;&gt;&gt; y = torch.tensor([3])</span>
<span class="sd">        &gt;&gt;&gt; # 0.25 * ((1-(0.8-0.1)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="sd">        &gt;&gt;&gt; loss(x, y)</span>
<span class="sd">        tensor(0.32...)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">int</span>

<div class="viewcode-block" id="MultiMarginLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiMarginLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;only p == 1 and p == 2 supported&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;MultiMarginLoss: expected weight to be None or 1D tensor, got </span><span class="si">{</span><span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span><span class="si">}</span><span class="s2">D instead&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span></div>


<div class="viewcode-block" id="MultiMarginLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.MultiMarginLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span>
            <span class="n">target</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
            <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span>
            <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="TripletMarginLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>


<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    The norm is calculated using the specified p value and a small constant :math:`\varepsilon` is</span>
<span class="sd">    added for numerical stability.</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the</span>
<span class="sd">    triplet margin loss for input tensors using a custom distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Default: :math:`1`.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.</span>
<span class="sd">        eps (float, optional): Small constant for numerical stability. Default: :math:`1e-6`.</span>
<span class="sd">        swap (bool, optional): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: ``False``.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when :attr:`reduce` is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` or :math:`(D)` where :math:`D` is the vector dimension.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;`` and</span>
<span class="sd">          input shape is :math:`(N, D)`; a scalar otherwise.</span>

<span class="sd">    Examples:</span>

<span class="sd">    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-7)</span>
<span class="sd">    &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;p&quot;</span><span class="p">,</span> <span class="s2">&quot;eps&quot;</span><span class="p">,</span> <span class="s2">&quot;swap&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

<div class="viewcode-block" id="TripletMarginLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span>
        <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">margin</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;TripletMarginLoss: expected margin to be greater than 0, got </span><span class="si">{</span><span class="n">margin</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span></div>


<div class="viewcode-block" id="TripletMarginLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span>
            <span class="n">anchor</span><span class="p">,</span>
            <span class="n">positive</span><span class="p">,</span>
            <span class="n">negative</span><span class="p">,</span>
            <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span>
            <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="TripletMarginWithDistanceLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginWithDistanceLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">TripletMarginWithDistanceLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given input</span>
<span class="sd">    tensors :math:`a`, :math:`p`, and :math:`n` (representing anchor,</span>
<span class="sd">    positive, and negative examples, respectively), and a nonnegative,</span>
<span class="sd">    real-valued function (&quot;distance function&quot;) used to compute the relationship</span>
<span class="sd">    between the anchor and positive example (&quot;positive distance&quot;) and the</span>
<span class="sd">    anchor and negative example (&quot;negative distance&quot;).</span>

<span class="sd">    The unreduced loss (i.e., with :attr:`reduction` set to ``&#39;none&#39;``)</span>
<span class="sd">    can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where :math:`N` is the batch size; :math:`d` is a nonnegative, real-valued function</span>
<span class="sd">    quantifying the closeness of two tensors, referred to as the :attr:`distance_function`;</span>
<span class="sd">    and :math:`margin` is a nonnegative margin representing the minimum difference</span>
<span class="sd">    between the positive and negative distances that is required for the loss to</span>
<span class="sd">    be 0.  The input tensors have :math:`N` elements each and can be of any shape</span>
<span class="sd">    that the distance function can handle.</span>

<span class="sd">    If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginLoss`, which computes the triplet</span>
<span class="sd">    loss for input tensors using the :math:`l_p` distance as the distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        distance_function (Callable, optional): A nonnegative, real-valued function that</span>
<span class="sd">            quantifies the closeness of two tensors. If not specified,</span>
<span class="sd">            `nn.PairwiseDistance` will be used.  Default: ``None``</span>
<span class="sd">        margin (float, optional): A nonnegative margin representing the minimum difference</span>
<span class="sd">            between the positive and negative distances required for the loss to be 0. Larger</span>
<span class="sd">            margins penalize cases where the negative examples are not distant enough from the</span>
<span class="sd">            anchors, relative to the positives. Default: :math:`1`.</span>
<span class="sd">        swap (bool, optional): Whether to use the distance swap described in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. If True, and if the positive example is closer to the</span>
<span class="sd">            negative example than the anchor is, swaps the positive example and the anchor in</span>
<span class="sd">            the loss computation. Default: ``False``.</span>
<span class="sd">        reduction (str, optional): Specifies the (optional) reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>


<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` represents any number of additional dimensions</span>
<span class="sd">          as supported by the distance function.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;``, or a scalar</span>
<span class="sd">          otherwise.</span>

<span class="sd">    Examples:</span>

<span class="sd">    &gt;&gt;&gt; # Initialize embeddings</span>
<span class="sd">    &gt;&gt;&gt; embedding = nn.Embedding(1000, 128)</span>
<span class="sd">    &gt;&gt;&gt; anchor_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; positive_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; negative_ids = torch.randint(0, 1000, (1,))</span>
<span class="sd">    &gt;&gt;&gt; anchor = embedding(anchor_ids)</span>
<span class="sd">    &gt;&gt;&gt; positive = embedding(positive_ids)</span>
<span class="sd">    &gt;&gt;&gt; negative = embedding(negative_ids)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Built-in Distance Function</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function</span>
<span class="sd">    &gt;&gt;&gt; def l_infinity(x1, x2):</span>
<span class="sd">    &gt;&gt;&gt;     return torch.max(torch.abs(x1 - x2), dim=1).values</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # xdoctest: +SKIP(&quot;FIXME: Would call backwards a second time&quot;)</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = (</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5))</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function (Lambda)</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = (</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(</span>
<span class="sd">    &gt;&gt;&gt;         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y)))</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        https://bmva-archive.org.uk/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;margin&quot;</span><span class="p">,</span> <span class="s2">&quot;swap&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

<div class="viewcode-block" id="TripletMarginWithDistanceLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginWithDistanceLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">distance_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">margin</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;TripletMarginWithDistanceLoss: expected margin to be greater than 0, got </span><span class="si">{</span><span class="n">margin</span><span class="si">}</span><span class="s2"> instead&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">distance_function</span> <span class="k">if</span> <span class="n">distance_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">PairwiseDistance</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span></div>


<div class="viewcode-block" id="TripletMarginWithDistanceLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.TripletMarginWithDistanceLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_with_distance_loss</span><span class="p">(</span>
            <span class="n">anchor</span><span class="p">,</span>
            <span class="n">positive</span><span class="p">,</span>
            <span class="n">negative</span><span class="p">,</span>
            <span class="n">distance_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">,</span>
            <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span>
            <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<div class="viewcode-block" id="CTCLoss">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CTCLoss">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">CTCLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Connectionist Temporal Classification loss.</span>

<span class="sd">    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the</span>
<span class="sd">    probability of possible alignments of input to target, producing a loss value which is differentiable</span>
<span class="sd">    with respect to each input node. The alignment of input to target is assumed to be &quot;many-to-one&quot;, which</span>
<span class="sd">    limits the length of the target sequence such that it must be :math:`\leq` the input length.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int, optional): blank label. Default :math:`0`.</span>
<span class="sd">        reduction (str, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the output losses will be divided by the target lengths and</span>
<span class="sd">            then the mean over the batch is taken, ``&#39;sum&#39;``: the output losses will be summed.</span>
<span class="sd">            Default: ``&#39;mean&#39;``</span>
<span class="sd">        zero_infinity (bool, optional):</span>
<span class="sd">            Whether to zero infinite losses and the associated gradients.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">            Infinite losses mainly occur when the inputs are too short</span>
<span class="sd">            to be aligned to the targets.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Log_probs: Tensor of size :math:`(T, N, C)` or :math:`(T, C)`,</span>
<span class="sd">          where :math:`T = \text{input length}`,</span>
<span class="sd">          :math:`N = \text{batch size}`, and</span>
<span class="sd">          :math:`C = \text{number of classes (including blank)}`.</span>
<span class="sd">          The logarithmized probabilities of the outputs (e.g. obtained with</span>
<span class="sd">          :func:`torch.nn.functional.log_softmax`).</span>
<span class="sd">        - Targets: Tensor of size :math:`(N, S)` or</span>
<span class="sd">          :math:`(\operatorname{sum}(\text{target\_lengths}))`,</span>
<span class="sd">          where :math:`N = \text{batch size}` and</span>
<span class="sd">          :math:`S = \text{max target length, if shape is } (N, S)`.</span>
<span class="sd">          It represents the target sequences. Each element in the target</span>
<span class="sd">          sequence is a class index. And the target index cannot be blank (default=0).</span>
<span class="sd">          In the :math:`(N, S)` form, targets are padded to the</span>
<span class="sd">          length of the longest sequence, and stacked.</span>
<span class="sd">          In the :math:`(\operatorname{sum}(\text{target\_lengths}))` form,</span>
<span class="sd">          the targets are assumed to be un-padded and</span>
<span class="sd">          concatenated within 1 dimension.</span>
<span class="sd">        - Input_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represents the lengths of the</span>
<span class="sd">          inputs (must each be :math:`\leq T`). And the lengths are specified</span>
<span class="sd">          for each sequence to achieve masking under the assumption that sequences</span>
<span class="sd">          are padded to equal lengths.</span>
<span class="sd">        - Target_lengths: Tuple or tensor of size :math:`(N)` or :math:`()`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represents lengths of the targets.</span>
<span class="sd">          Lengths are specified for each sequence to achieve masking under the</span>
<span class="sd">          assumption that sequences are padded to equal lengths. If target shape is</span>
<span class="sd">          :math:`(N,S)`, target_lengths are effectively the stop index</span>
<span class="sd">          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for</span>
<span class="sd">          each target in a batch. Lengths must each be :math:`\leq S`</span>
<span class="sd">          If the targets are given as a 1d tensor that is the concatenation of individual</span>
<span class="sd">          targets, the target_lengths must add up to the total length of the tensor.</span>
<span class="sd">        - Output: scalar if :attr:`reduction` is ``&#39;mean&#39;`` (default) or</span>
<span class="sd">          ``&#39;sum&#39;``. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)` if input is batched or</span>
<span class="sd">          :math:`()` if input is unbatched, where :math:`N = \text{batch size}`.</span>

<span class="sd">    Examples:</span>

<span class="sd">        &gt;&gt;&gt; # Target are to be padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50  # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20  # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16  # Batch size</span>
<span class="sd">        &gt;&gt;&gt; S = 30  # Target sequence length of longest target in batch (padding length)</span>
<span class="sd">        &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(</span>
<span class="sd">        ...     low=S_min,</span>
<span class="sd">        ...     high=S,</span>
<span class="sd">        ...     size=(N,),</span>
<span class="sd">        ...     dtype=torch.long,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Target are to be un-padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50  # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20  # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16  # Batch size</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(</span>
<span class="sd">        ...     low=1,</span>
<span class="sd">        ...     high=C,</span>
<span class="sd">        ...     size=(sum(target_lengths),),</span>
<span class="sd">        ...     dtype=torch.long,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Target are to be un-padded and unbatched (effectively N=1)</span>
<span class="sd">        &gt;&gt;&gt; T = 50  # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20  # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,C)</span>
<span class="sd">        &gt;&gt;&gt; # xdoctest: +SKIP(&quot;FIXME: error in doctest&quot;)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, C).log_softmax(1).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.tensor(T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(</span>
<span class="sd">        ...     low=1,</span>
<span class="sd">        ...     high=C,</span>
<span class="sd">        ...     size=(target_lengths,),</span>
<span class="sd">        ...     dtype=torch.long,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        A. Graves et al.: Connectionist Temporal Classification:</span>
<span class="sd">        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:</span>
<span class="sd">        https://www.cs.toronto.edu/~graves/icml_2006.pdf</span>

<span class="sd">    Note:</span>
<span class="sd">        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be</span>
<span class="sd">        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,</span>
<span class="sd">        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of</span>
<span class="sd">        dtype :attr:`torch.int32`.</span>

<span class="sd">        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.</span>


<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;blank&quot;</span><span class="p">,</span> <span class="s2">&quot;reduction&quot;</span><span class="p">]</span>
    <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span>

<div class="viewcode-block" id="CTCLoss.__init__">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CTCLoss.__init__">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blank</span> <span class="o">=</span> <span class="n">blank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span> <span class="o">=</span> <span class="n">zero_infinity</span></div>


<div class="viewcode-block" id="CTCLoss.forward">
<a class="viewcode-back" href="../../../../api/losses.html#torchium.losses.CTCLoss.forward">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">log_probs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">input_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">target_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span>
            <span class="n">log_probs</span><span class="p">,</span>
            <span class="n">targets</span><span class="p">,</span>
            <span class="n">input_lengths</span><span class="p">,</span>
            <span class="n">target_lengths</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span><span class="p">,</span>
        <span class="p">)</span></div>
</div>



<span class="c1"># TODO: L1HingeEmbeddingCriterion</span>
<span class="c1"># TODO: MSECriterion weight</span>
<span class="c1"># TODO: ClassSimplexCriterion</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Torchium Contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>